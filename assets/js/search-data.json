{
  
    
        "post0": {
            "title": "Covid vs India",
            "content": "%of free vs paid ,( piechart ?) | how many centers per state and who has best ratio against population? ( chart of top 5 and show it on map ) | at current allocation how many days will it take to vaccinate 18+? | what&#39;s ground reality for Key states like Delhi, Maharastra , Tamil Nadu | What&#39;s ground reality for populus states like UP, MP, | How does it fare around | Latest vaccine slots per district and State in India | Top 5 States Running behind on schedule | Top 5 States Running ahead of everyone else. | When can I latest find slots in Delhi, Bombay, Chennai and Bangalore for people between 18-45 | Thanks to https://github.com/bhattbhavesh91/cowin-vaccination-slot-availability/blob/main/cowin-api-availability.ipynb for doing actual work, I picked up loads of stuff from there. . Get All libraries in place . import geopandas as gpd import pandas as pd import altair as alt import requests import json from collections import defaultdict from dataclasses import dataclass, asdict, field from datetime import datetime, timedelta from typing import List import uuid . Lets make a data class to store our geographical and vaccination data . @dataclass class District: district_id:int = None district_name:str = None state_id:int = None district_created_at: datetime = field(default_factory=datetime.now) @dataclass class Session: session_uuid:str = None date:datetime = None query_date:datetime= None available_capacity:int = None min_age_limit:int = None vaccine:str = None center_id:str = None district_id:str = None session_created_at: datetime = field(default_factory=datetime.now) @dataclass class Center: center_uuid:str = None center_id:int = None center_name:str = None state_name:str = None district_name:str = None block_name:str = None pincode:str = None lat:int = None lng:int = None from_hour:datetime = None to_hour:datetime = None fee_type:str = None district_id:str = None center_created_at: datetime = field(default_factory=datetime.now) @dataclass class NoSlotAvailable: district_id:str = None date:datetime = None no_slot_available_created_at: datetime = field(default_factory=datetime.now) . Lets call the API to get the populate geographical data . MOZILLA_HEADER = &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36&#39; GET_DISTRICT_DATA_API_URL = &quot;https://cdn-api.co-vin.in/api/v2/admin/location/districts/{}&quot; GET_APOINTMENT_DATA_API_URL = &quot;https://cdn-api.co-vin.in/api/v2/appointment/sessions/public/calendarByDistrict?district_id={}&amp;date={}&quot; DISTRICTS = defaultdict(District) CENTERS = defaultdict(Center) SESSIONS = defaultdict(Session) NO_SLOT_AVAILABLE = [] MAX_NUMBER_OF_STATES = 40 for state_code in range(1, MAX_NUMBER_OF_STATES): headers = {&#39;User-Agent&#39;: MOZILLA_HEADER} response = requests.get(GET_DISTRICT_DATA_API_URL.format(state_code), headers=headers) districts_data = json.loads(response.content) for district in districts_data[&#39;districts&#39;]: district_name = district[&#39;district_name&#39;] district_id = district[&#39;district_id&#39;] district = District(district_name = district_name, district_id = district_id , state_id=state_code) DISTRICTS[district_id] = district . DISTRICT_DF = pd.DataFrame.from_dict([asdict(district) for district in DISTRICTS.values()]) . Lets now call actual api to get slots . MAX_DAYS = 7 #This step takes loads of time def get_days_in_future_from_today(): base = datetime.today() date_list = [base + timedelta(days=x) for x in range(MAX_DAYS)] return [x.strftime(&quot;%d-%m-%Y&quot;) for x in date_list] for district_id in DISTRICTS.keys(): print(&quot;District {}&quot;.format(district_id)) for slot_date in get_days_in_future_from_today(): URL = GET_APOINTMENT_DATA_API_URL.format(district_id, slot_date) response = requests.get(URL) if response.ok: resp_json = response.json() if resp_json[&quot;centers&quot;]: for center in resp_json[&quot;centers&quot;]: center_uuid = str(uuid.uuid4()) center_id = center[&quot;center_id&quot;] center_name = center[&quot;name&quot;] CENTERS[center_uuid] = Center(center_uuid=center_uuid, center_id=center_id, center_name=center_name, lat=center[&quot;lat&quot;], lng=center[&quot;long&quot;], from_hour=center[&quot;from&quot;], to_hour=center[&quot;to&quot;], district_id=district_id, state_name=center[&quot;state_name&quot;], district_name=center[&quot;district_name&quot;], block_name=center[&quot;block_name&quot;], pincode=center[&quot;pincode&quot;], fee_type=center[&quot;fee_type&quot;]) for session in center[&quot;sessions&quot;]: session_id = session[&quot;session_id&quot;] SESSIONS[session_id] = Session(session_uuid=session_id, date=session[&quot;date&quot;], query_date=slot_date, available_capacity=session[&quot;available_capacity&quot;], min_age_limit=session[&quot;min_age_limit&quot;], vaccine=session[&quot;vaccine&quot;], district_id=district_id, center_id=center_id) else: NO_SLOT_AVAILABLE.append(NoSlotAvailable(district_id=district_id, date=slot_date)) # print(&quot;No slot on {} in district {}&quot;.format(slot_date, district_id)) . CENTER_DF = pd.DataFrame.from_dict([asdict(district) for district in CENTERS.values()]) SESSION_DF = pd.DataFrame.from_dict([asdict(session) for session in SESSIONS.values()]) NO_SLOT_AVAILABLE_DF = pd.DataFrame.from_dict([asdict(no_slot_available) for no_slot_available in NO_SLOT_AVAILABLE]) . NO_SLOT_AVAILABLE_DF.head() . district_id date no_slot_available_created_at . 0 5 | 04-05-2021 | 2021-05-03 16:22:33.578494 | . 1 5 | 05-05-2021 | 2021-05-03 16:22:33.744831 | . 2 5 | 06-05-2021 | 2021-05-03 16:22:33.859553 | . 3 5 | 07-05-2021 | 2021-05-03 16:22:33.970119 | . 4 5 | 08-05-2021 | 2021-05-03 16:22:34.321653 | . SESSION_DF.head() . session_uuid date query_date available_capacity min_age_limit vaccine center_id district_id session_created_at . 0 207fd35f-888d-460d-8304-f0b99f9a13a0 | 03-05-2021 | 03-05-2021 | 49.0 | 45 | | 570779 | 3 | 2021-05-03 16:22:25.027502 | . 1 88373311-b0da-40f6-bbea-ee029f86eb1c | 04-05-2021 | 04-05-2021 | 50.0 | 45 | | 570779 | 3 | 2021-05-03 16:22:25.264174 | . 2 8ef1ec99-7318-4cb6-bc66-9ed3675f9a48 | 05-05-2021 | 05-05-2021 | 50.0 | 45 | | 570779 | 3 | 2021-05-03 16:22:25.412119 | . 3 bd311aec-c4cf-45f3-8f06-2dbc7b634de4 | 06-05-2021 | 06-05-2021 | 50.0 | 45 | | 570779 | 3 | 2021-05-03 16:22:25.628695 | . 4 f6022b09-7079-43fe-a48c-dc82e04a0058 | 08-05-2021 | 08-05-2021 | 49.0 | 45 | | 570779 | 3 | 2021-05-03 16:22:25.982494 | . CENTER_DF.head() . center_uuid center_id center_name state_name district_name block_name pincode lat lng from_hour to_hour fee_type district_id center_created_at . 0 68709bda-d461-4295-b453-0446e3c3836d | 570779 | BJR Hospital | Andaman and Nicobar Islands | Nicobar | Car Nicobar | 744301 | 0 | 0 | 09:00:00 | 17:00:00 | Free | 3 | 2021-05-03 16:22:25.027457 | . 1 39004cf0-9221-4b91-a3ed-efbd4d129d25 | 639974 | MI Room DHQ 10 | Andaman and Nicobar Islands | Nicobar | Campbell Bay | 744302 | 7 | 93 | 09:00:00 | 18:00:00 | Free | 3 | 2021-05-03 16:22:25.027546 | . 2 4b6ad957-cc5c-489b-8b40-028512b83dbf | 552109 | Campbellbay PHC | Andaman and Nicobar Islands | Nicobar | Campbell Bay | 744302 | 7 | 93 | 09:00:00 | 17:00:00 | Free | 3 | 2021-05-03 16:22:25.027647 | . 3 18036b66-4a41-4424-b5e5-672f5722ce6f | 552108 | Nancowry CHC | Andaman and Nicobar Islands | Nicobar | Nancowry | 744303 | 7 | 93 | 09:00:00 | 17:00:00 | Free | 3 | 2021-05-03 16:22:25.027704 | . 4 e44f0c92-4368-4a4e-b0de-020095e84dfb | 639986 | SMC 37 WING | Andaman and Nicobar Islands | Nicobar | Car Nicobar | 744301 | 9 | 92 | 09:00:00 | 18:00:00 | Free | 3 | 2021-05-03 16:22:25.027753 | . DISTRICT_DF.head() . district_id district_name state_id district_created_at . 0 3 | Nicobar | 1 | 2021-05-03 16:22:15.927874 | . 1 1 | North and Middle Andaman | 1 | 2021-05-03 16:22:15.927885 | . 2 2 | South Andaman | 1 | 2021-05-03 16:22:15.927887 | . 3 9 | Anantapur | 2 | 2021-05-03 16:22:17.377390 | . 4 10 | Chittoor | 2 | 2021-05-03 16:22:17.377401 | . session_center_merged_df = pd.merge(SESSION_DF, CENTER_DF, on=&quot;center_id&quot;) session_center_district_df = pd.merge(session_center_merged_df, DISTRICT_DF, left_on=&#39;district_id_x&#39;, right_on=&quot;district_id&quot;) . session_center_district_df.to_csv(&quot;vaccination_slot_data.csv&quot;) session_center_district_df . session_uuid date query_date available_capacity min_age_limit vaccine center_id district_id_x session_created_at center_uuid ... lng from_hour to_hour fee_type district_id_y center_created_at district_id district_name_y state_id district_created_at . 0 207fd35f-888d-460d-8304-f0b99f9a13a0 | 03-05-2021 | 03-05-2021 | 49.0 | 45 | | 570779 | 3 | 2021-05-03 16:22:25.027502 | 68709bda-d461-4295-b453-0446e3c3836d | ... | 0 | 09:00:00 | 17:00:00 | Free | 3 | 2021-05-03 16:22:25.027457 | 3 | Nicobar | 1 | 2021-05-03 16:22:15.927874 | . 1 207fd35f-888d-460d-8304-f0b99f9a13a0 | 03-05-2021 | 03-05-2021 | 49.0 | 45 | | 570779 | 3 | 2021-05-03 16:22:25.027502 | 60d78a27-077d-4d9e-b3a7-f0d78907f375 | ... | 0 | 09:00:00 | 17:00:00 | Free | 3 | 2021-05-03 16:22:25.264162 | 3 | Nicobar | 1 | 2021-05-03 16:22:15.927874 | . 2 207fd35f-888d-460d-8304-f0b99f9a13a0 | 03-05-2021 | 03-05-2021 | 49.0 | 45 | | 570779 | 3 | 2021-05-03 16:22:25.027502 | 87a707cc-6929-4ff3-94cf-e0c7d5dfe750 | ... | 0 | 09:00:00 | 17:00:00 | Free | 3 | 2021-05-03 16:22:25.412101 | 3 | Nicobar | 1 | 2021-05-03 16:22:15.927874 | . 3 207fd35f-888d-460d-8304-f0b99f9a13a0 | 03-05-2021 | 03-05-2021 | 49.0 | 45 | | 570779 | 3 | 2021-05-03 16:22:25.027502 | 887d3e0a-fd2a-4f69-9af7-9e8ce874d155 | ... | 0 | 09:00:00 | 17:00:00 | Free | 3 | 2021-05-03 16:22:25.628681 | 3 | Nicobar | 1 | 2021-05-03 16:22:15.927874 | . 4 207fd35f-888d-460d-8304-f0b99f9a13a0 | 03-05-2021 | 03-05-2021 | 49.0 | 45 | | 570779 | 3 | 2021-05-03 16:22:25.027502 | f4d8eb46-0c16-4f88-8afa-6bc40c91b2e0 | ... | 0 | 09:00:00 | 17:00:00 | Free | 3 | 2021-05-03 16:22:25.781478 | 3 | Nicobar | 1 | 2021-05-03 16:22:15.927874 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1251756 aa1390fe-2b5f-46c3-aab1-8807035a210b | 04-05-2021 | 04-05-2021 | 10.0 | 45 | | 343857 | 139 | 2021-05-03 16:44:00.778635 | ee1be1a2-ff91-4362-9cd8-68aa7492545c | ... | 70 | 09:00:00 | 17:00:00 | Free | 139 | 2021-05-03 16:44:00.778626 | 139 | Diu | 37 | 2021-05-03 16:22:24.553471 | . 1251757 aa1390fe-2b5f-46c3-aab1-8807035a210b | 04-05-2021 | 04-05-2021 | 10.0 | 45 | | 343857 | 139 | 2021-05-03 16:44:00.778635 | c31e86c7-cb3f-4be5-9fcd-1cb1f3bacd3a | ... | 70 | 09:00:00 | 17:00:00 | Free | 139 | 2021-05-03 16:44:00.927269 | 139 | Diu | 37 | 2021-05-03 16:22:24.553471 | . 1251758 56c835d4-b1be-4df7-97de-20b88f3d61ee | 05-05-2021 | 05-05-2021 | 10.0 | 45 | | 343857 | 139 | 2021-05-03 16:44:00.927278 | 9143e567-5417-4623-abc1-ce6235b5f905 | ... | 70 | 09:00:00 | 17:00:00 | Free | 139 | 2021-05-03 16:44:00.663340 | 139 | Diu | 37 | 2021-05-03 16:22:24.553471 | . 1251759 56c835d4-b1be-4df7-97de-20b88f3d61ee | 05-05-2021 | 05-05-2021 | 10.0 | 45 | | 343857 | 139 | 2021-05-03 16:44:00.927278 | ee1be1a2-ff91-4362-9cd8-68aa7492545c | ... | 70 | 09:00:00 | 17:00:00 | Free | 139 | 2021-05-03 16:44:00.778626 | 139 | Diu | 37 | 2021-05-03 16:22:24.553471 | . 1251760 56c835d4-b1be-4df7-97de-20b88f3d61ee | 05-05-2021 | 05-05-2021 | 10.0 | 45 | | 343857 | 139 | 2021-05-03 16:44:00.927278 | c31e86c7-cb3f-4be5-9fcd-1cb1f3bacd3a | ... | 70 | 09:00:00 | 17:00:00 | Free | 139 | 2021-05-03 16:44:00.927269 | 139 | Diu | 37 | 2021-05-03 16:22:24.553471 | . 1251761 rows × 26 columns . session_center_district_df[[&#39;date&#39;, &#39;state_name&#39;]].groupby(&#39;state_name&#39;).apply(lambda x : x.sort_values(by = &#39;date&#39;, ascending = True).head(3).reset_index(drop = True)) . date state_name . state_name . Andaman and Nicobar Islands 0 03-05-2021 | Andaman and Nicobar Islands | . 1 03-05-2021 | Andaman and Nicobar Islands | . 2 03-05-2021 | Andaman and Nicobar Islands | . Andhra Pradesh 0 03-05-2021 | Andhra Pradesh | . 1 03-05-2021 | Andhra Pradesh | . ... ... ... | ... | . Uttarakhand 1 03-05-2021 | Uttarakhand | . 2 03-05-2021 | Uttarakhand | . West Bengal 0 03-05-2021 | West Bengal | . 1 03-05-2021 | West Bengal | . 2 03-05-2021 | West Bengal | . 111 rows × 2 columns . sessions_for_below_45 = session_center_district_df [session_center_district_df[&quot;min_age_limit&quot;] &lt;45] sessions_for_below_45_free_fee_type_count = len(sessions_for_below_45[sessions_for_below_45[&quot;fee_type&quot;]==&quot;Free&quot;]) sessions_for_below_45_not_free_fee_type_count = len(sessions_for_below_45) - sessions_for_below_45_free_fee_type_count sessions_for_below_45_not_free_fee_type_count, sessions_for_below_45_free_fee_type_count source = pd.DataFrame({ &quot;y&quot;:[&quot;Free Vaccines Centers&quot;,&quot;Paid Vaccines Centers&quot;], &quot;x&quot;:[sessions_for_below_45_not_free_fee_type_count,sessions_for_below_45_free_fee_type_count] }) bars = alt.Chart(source).transform_joinaggregate( TotalX=&#39;sum(x)&#39;, ).transform_calculate( PercentOfTotal=&quot;datum.x / datum.TotalX&quot; ).mark_bar( cornerRadiusTopLeft=3, cornerRadiusTopRight=3 ).encode( alt.X(&#39;PercentOfTotal:Q&#39;, axis=alt.Axis(format=&#39;.0%&#39;)), y=&#39;y&#39;, ) text = bars.mark_text( align=&#39;left&#39;, baseline=&#39;middle&#39;, dx=3 # Nudges text to right so it doesn&#39;t appear on top of the bar ).encode( text=&#39;x&#39; ) (bars+text).properties(height=200) .",
            "url": "https://shashanksingh.github.io/blog_redux/dfs/jupyter/python/2021/04/30/covid-vacctionations-slots-in-india.html",
            "relUrl": "/dfs/jupyter/python/2021/04/30/covid-vacctionations-slots-in-india.html",
            "date": " • Apr 30, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Covid vs The World - Part II",
            "content": "During this madness of ever locking down and then un-locked down&#39;s, I have been wondering how the world has been affected by the madness of it all. So I did, what I did best, try searching for some answers by experimenting with data that I found online. . Probably not the best way, but served as a good proxy for me to form a mental picture of how the world is dragging itself through 2020. . On more positive news with vaccine just around the corner, there is finally light at end of the tunnel. . So to cut short to the chase, I am using google trend and timelines from matplotlib to see what correlations I could find. The adage applies here &quot;correlation is not causation&quot;, so take it with pinch of salt. . ticket_google_trend_per_week_df = pd.DataFrame(ticket_google_trend_per_week, columns= [&quot;week&quot;,&quot;ticket&quot;, &quot;vaccine&quot;, &quot;stream&quot;]) ticket_google_trend_per_week_df.head() . week ticket vaccine stream . 0 2019-11-17 | 66 | 12 | 67 | . 1 2019-11-24 | 66 | 10 | 67 | . 2 2019-12-01 | 67 | 11 | 72 | . 3 2019-12-08 | 65 | 10 | 61 | . 4 2019-12-15 | 65 | 10 | 69 | . ticket_google_trend_per_week_melted_df = pd.melt(ticket_google_trend_per_week_df,id_vars=[&quot;week&quot;],var_name=[&quot;types&quot;]) alt.Chart(ticket_google_trend_per_week_melted_df).mark_area(opacity=0.3).encode( x = &#39;week&#39;, y = alt.Y(&quot;value:Q&quot;, stack=None), color=&#39;types&#39;, ).properties( height=400, width=850, ) . So my inference was the moment Covid-19 hit prime time globally, people stopped looking for tickets, top search items being: the ticket price, ticket booking, train ticket, online ticket, ticket flight, etc. Consequently, a global lockdown let everyone focus more on vaccine news and the top search item being covid vaccine and started munching on online stream whatever they could, top search item being: live stream, stream on, how to stream, tv stream, etc. . I pulled data from here. . The date all this started happening, 1st of march 2020. .",
            "url": "https://shashanksingh.github.io/blog_redux/dfs/jupyter/python/2020/11/01/covid-impact-on-industries-based-on-google-trends.html",
            "relUrl": "/dfs/jupyter/python/2020/11/01/covid-impact-on-industries-based-on-google-trends.html",
            "date": " • Nov 1, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Depth First Search : Planning a vacation",
            "content": "What possible ways are there to reach from DARWIN to SYDNEY ? . import requests import pandas as pd import json r = requests.get(&#39;https://data.gov.au/data/api/3/action/datastore_search?resource_id=677d307f-6a1f-4de4-9b85-5e1aa7074423&#39;) data_json = json.loads(r.text) data = pd.DataFrame(data_json[&quot;result&quot;][&quot;records&quot;],columns=[&quot;City1&quot;,&quot;City2&quot;,&quot;Aircraft_Trips&quot;]) . pd.set_option(&quot;display.max_rows&quot;, 10) data.groupby(by = [&quot;City1&quot;,&quot;City2&quot;]).count() . Aircraft_Trips . City1 City2 . ADELAIDE ALICE SPRINGS 2 | . BRISBANE 3 | . DARWIN 4 | . GOLD COAST 4 | . MELBOURNE 1 | . ... ... ... | . PORT MACQUARIE SYDNEY 1 | . PROSERPINE SYDNEY 1 | . SUNSHINE COAST MELBOURNE 1 | . SYDNEY TOWNSVILLE 2 | . WAGGA WAGGA 1 | . 49 rows × 1 columns . #first initizlie it unique_columns = list(set(data.City1.unique()).union(set(data.City2.unique()))) adjacency_matrix = [[0 for columns in range(len(unique_columns))] for row in range(len(unique_columns))] adjacency_list = [] #Lets create adjcacncey network # fill in the values for ind in data.index: city1_index = unique_columns.index(data[&#39;City1&#39;][ind]) city2_index = unique_columns.index(data[&#39;City2&#39;][ind]) adjacency_matrix[city1_index][city2_index] = 1 adjacency_matrix[city2_index][city1_index] = 1 draw_graph_from_adjacency_matrix(adjacency_matrix) . from typing import List TOTAL_VERTICES = len(adjacency_list) TOTAL_NODES = len(unique_columns) class PathFinder(): def __init__(self, adjacency_matrix : List[List[int]])-&gt; None: self.adjacency_matrix = adjacency_matrix def all_paths_helper(self, city_name_from:str , city_name_to:str, visited: List[bool], path: List[str], all_paths: List[List[str]]) -&gt; str: city_name_from_index = unique_columns.index(city_name_from) visited[city_name_from_index] = True path.append(city_name_from) if city_name_from == city_name_to : all_paths.append(path.copy()) # print(path) else: neighbours = self.adjacency_matrix[city_name_from_index:city_name_from_index+1][0] for index in range(len(neighbours)): if neighbours[index] == 1: neighbour = unique_columns[index] if visited[index] == False: self.all_paths_helper(neighbour, city_name_to, visited, path, all_paths) #for all neighbours of this path call ourselbes path.pop() visited[city_name_from_index]= False def all_paths(self, city_name_from:str , city_name_to:str) -&gt; str: if not (city_name_from in unique_columns and city_name_to in unique_columns): return &quot;Incorrect City&quot; visited = [False] * TOTAL_NODES path = [] all_paths = [] self.all_paths_helper(city_name_from , city_name_to, visited, path , all_paths) return all_paths pf = PathFinder(adjacency_matrix) all_paths = pf.all_paths(&quot;DARWIN&quot;, &quot;SYDNEY&quot;) . df_all_paths = pd.DataFrame(all_paths) df_all_paths . 0 1 2 3 4 5 6 7 8 . 0 DARWIN | MELBOURNE | SYDNEY | None | None | None | None | None | None | . 1 DARWIN | MELBOURNE | CANBERRA | SYDNEY | None | None | None | None | None | . 2 DARWIN | MELBOURNE | CANBERRA | BRISBANE | PROSERPINE | SYDNEY | None | None | None | . 3 DARWIN | MELBOURNE | CANBERRA | BRISBANE | SYDNEY | None | None | None | None | . 4 DARWIN | MELBOURNE | CANBERRA | BRISBANE | CAIRNS | SYDNEY | None | None | None | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 359 DARWIN | ADELAIDE | PERTH | BRISBANE | CAIRNS | MELBOURNE | ALICE SPRINGS | SYDNEY | None | . 360 DARWIN | ADELAIDE | PERTH | BRISBANE | CAIRNS | MELBOURNE | NEWCASTLE | GOLD COAST | SYDNEY | . 361 DARWIN | ADELAIDE | PERTH | BRISBANE | CAIRNS | MELBOURNE | GOLD COAST | SYDNEY | None | . 362 DARWIN | ADELAIDE | PERTH | BRISBANE | CAIRNS | SYDNEY | None | None | None | . 363 DARWIN | ADELAIDE | PERTH | BRISBANE | TOWNSVILLE | SYDNEY | None | None | None | . 364 rows × 9 columns .",
            "url": "https://shashanksingh.github.io/blog_redux/dfs/jupyter/python/2020/10/03/dfs.html",
            "relUrl": "/dfs/jupyter/python/2020/10/03/dfs.html",
            "date": " • Oct 3, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Covid vs The World - Part I",
            "content": "%matplotlib inline import matplotlib.pyplot as plt import pandas as pd #### -- Step 1 (Download data)- URL_DATASET = r&#39;https://raw.githubusercontent.com/datasets/covid-19/master/data/countries-aggregated.csv&#39; df = pd.read_csv(URL_DATASET) df[&#39;NewCases&#39;] = df[&#39;Confirmed&#39;] - df[&#39;Confirmed&#39;].shift(1) #### -- Step 2 (Select data for India)- df_india = df[df[&#39;Country&#39;] == &#39;India&#39;] df_india.tail(5) . Date Country Confirmed Recovered Deaths NewCases . 25275 2020-11-28 | India | 9392919 | 8802267 | 136696 | 41810.0 | . 25276 2020-11-29 | India | 9431691 | 8847600 | 137139 | 38772.0 | . 25277 2020-11-30 | India | 9462809 | 8889585 | 137621 | 31118.0 | . 25278 2020-12-01 | India | 9499413 | 8932647 | 138122 | 36604.0 | . 25279 2020-12-02 | India | 9534964 | 8973373 | 138648 | 35551.0 | . #### -- Step 3 (Plot data)- # Increase size of plot plt.rcParams[&quot;figure.figsize&quot;]=20,14 # Remove if not on Jupyter df_india.plot( x = &#39;Date&#39;, y = &#39;Confirmed&#39;, color = &#39;blue&#39;) df_india.plot( x = &#39;Date&#39;, y = &#39;NewCases&#39;, color = &#39;blue&#39;) . &lt;AxesSubplot:xlabel=&#39;Date&#39;&gt; .",
            "url": "https://shashanksingh.github.io/blog_redux/covid/jupyter/python/2020/09/20/covid-impact.html",
            "relUrl": "/covid/jupyter/python/2020/09/20/covid-impact.html",
            "date": " • Sep 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "",
          "url": "https://shashanksingh.github.io/blog_redux/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://shashanksingh.github.io/blog_redux/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}