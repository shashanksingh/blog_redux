{
  
    
        "post0": {
            "title": "Covid vs The World - Part II",
            "content": "During this madness of ever locking down and then un-locked down&#39;s, I have been wondering how the world has been affected by the madness of it all. So I did, what I did best, try searching for some answers by experimenting with data that I found online. . Probably not the best way, but served as a good proxy for me to form a mental picture of how the world is dragging itself through 2020. . On more positive news with vaccine just around the corner, there is finally light at end of the tunnel. . So to cut short to the chase, I am using google trend and timelines from matplotlib to see what correlations I could find. The adage applies here &quot;correlation is not causation&quot;, so take it with pinch of salt. . ticket_google_trend_per_week_df = pd.DataFrame(ticket_google_trend_per_week, columns= [&quot;week&quot;,&quot;ticket&quot;, &quot;vaccine&quot;, &quot;stream&quot;]) ticket_google_trend_per_week_df.head() . week ticket vaccine stream . 0 2019-11-17 | 66 | 12 | 67 | . 1 2019-11-24 | 66 | 10 | 67 | . 2 2019-12-01 | 67 | 11 | 72 | . 3 2019-12-08 | 65 | 10 | 61 | . 4 2019-12-15 | 65 | 10 | 69 | . ticket_google_trend_per_week_melted_df = pd.melt(ticket_google_trend_per_week_df,id_vars=[&quot;week&quot;],var_name=[&quot;types&quot;]) alt.Chart(ticket_google_trend_per_week_melted_df).mark_area(opacity=0.3).encode( x = &#39;week&#39;, y = alt.Y(&quot;value:Q&quot;, stack=None), color=&#39;types&#39;, ).properties( height=400, width=850, ) . So my inference was the moment Covid-19 hit prime time globally, people stopped looking for tickets, top search items being: the ticket price, ticket booking, train ticket, online ticket, ticket flight, etc. Consequently, a global panic let everyone focus more on vaccine news and the top search item being covid vaccine and started munching on online stream whatever they could, top search item being: live stream, stream on, how to stream, tv stream, etc. . I pulled data from here. . The date all this started happening, 1st of march 2020. .",
            "url": "https://shashanksingh.github.io/blog_redux/dfs/jupyter/python/2020/11/01/covid-impact-on-industries-based-on-google-trends.html",
            "relUrl": "/dfs/jupyter/python/2020/11/01/covid-impact-on-industries-based-on-google-trends.html",
            "date": " • Nov 1, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Depth First Search : Planning a vacation",
            "content": "What possible ways are there to reach from DARWIN to SYDNEY ? . import requests import pandas as pd import json r = requests.get(&#39;https://data.gov.au/data/api/3/action/datastore_search?resource_id=677d307f-6a1f-4de4-9b85-5e1aa7074423&#39;) data_json = json.loads(r.text) data = pd.DataFrame(data_json[&quot;result&quot;][&quot;records&quot;],columns=[&quot;City1&quot;,&quot;City2&quot;,&quot;Aircraft_Trips&quot;]) . pd.set_option(&quot;display.max_rows&quot;, 10) data.groupby(by = [&quot;City1&quot;,&quot;City2&quot;]).count() . Aircraft_Trips . City1 City2 . ADELAIDE ALICE SPRINGS 2 | . BRISBANE 3 | . DARWIN 4 | . GOLD COAST 4 | . MELBOURNE 1 | . ... ... ... | . PORT MACQUARIE SYDNEY 1 | . PROSERPINE SYDNEY 1 | . SUNSHINE COAST MELBOURNE 1 | . SYDNEY TOWNSVILLE 2 | . WAGGA WAGGA 1 | . 49 rows × 1 columns . #first initizlie it unique_columns = list(set(data.City1.unique()).union(set(data.City2.unique()))) adjacency_matrix = [[0 for columns in range(len(unique_columns))] for row in range(len(unique_columns))] adjacency_list = [] #Lets create adjcacncey network # fill in the values for ind in data.index: city1_index = unique_columns.index(data[&#39;City1&#39;][ind]) city2_index = unique_columns.index(data[&#39;City2&#39;][ind]) adjacency_matrix[city1_index][city2_index] = 1 adjacency_matrix[city2_index][city1_index] = 1 draw_graph_from_adjacency_matrix(adjacency_matrix) . from typing import List TOTAL_VERTICES = len(adjacency_list) TOTAL_NODES = len(unique_columns) class PathFinder(): def __init__(self, adjacency_matrix : List[List[int]])-&gt; None: self.adjacency_matrix = adjacency_matrix def all_paths_helper(self, city_name_from:str , city_name_to:str, visited: List[bool], path: List[str], all_paths: List[List[str]]) -&gt; str: city_name_from_index = unique_columns.index(city_name_from) visited[city_name_from_index] = True path.append(city_name_from) if city_name_from == city_name_to : all_paths.append(path.copy()) # print(path) else: neighbours = self.adjacency_matrix[city_name_from_index:city_name_from_index+1][0] for index in range(len(neighbours)): if neighbours[index] == 1: neighbour = unique_columns[index] if visited[index] == False: self.all_paths_helper(neighbour, city_name_to, visited, path, all_paths) #for all neighbours of this path call ourselbes path.pop() visited[city_name_from_index]= False def all_paths(self, city_name_from:str , city_name_to:str) -&gt; str: if not (city_name_from in unique_columns and city_name_to in unique_columns): return &quot;Incorrect City&quot; visited = [False] * TOTAL_NODES path = [] all_paths = [] self.all_paths_helper(city_name_from , city_name_to, visited, path , all_paths) return all_paths pf = PathFinder(adjacency_matrix) all_paths = pf.all_paths(&quot;DARWIN&quot;, &quot;SYDNEY&quot;) . df_all_paths = pd.DataFrame(all_paths) df_all_paths . 0 1 2 3 4 5 6 7 8 . 0 DARWIN | MELBOURNE | SYDNEY | None | None | None | None | None | None | . 1 DARWIN | MELBOURNE | CANBERRA | SYDNEY | None | None | None | None | None | . 2 DARWIN | MELBOURNE | CANBERRA | BRISBANE | PROSERPINE | SYDNEY | None | None | None | . 3 DARWIN | MELBOURNE | CANBERRA | BRISBANE | SYDNEY | None | None | None | None | . 4 DARWIN | MELBOURNE | CANBERRA | BRISBANE | CAIRNS | SYDNEY | None | None | None | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 359 DARWIN | ADELAIDE | PERTH | BRISBANE | CAIRNS | MELBOURNE | ALICE SPRINGS | SYDNEY | None | . 360 DARWIN | ADELAIDE | PERTH | BRISBANE | CAIRNS | MELBOURNE | NEWCASTLE | GOLD COAST | SYDNEY | . 361 DARWIN | ADELAIDE | PERTH | BRISBANE | CAIRNS | MELBOURNE | GOLD COAST | SYDNEY | None | . 362 DARWIN | ADELAIDE | PERTH | BRISBANE | CAIRNS | SYDNEY | None | None | None | . 363 DARWIN | ADELAIDE | PERTH | BRISBANE | TOWNSVILLE | SYDNEY | None | None | None | . 364 rows × 9 columns .",
            "url": "https://shashanksingh.github.io/blog_redux/dfs/jupyter/python/2020/10/03/dfs.html",
            "relUrl": "/dfs/jupyter/python/2020/10/03/dfs.html",
            "date": " • Oct 3, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Covid vs The World - Part I",
            "content": "%matplotlib inline import matplotlib.pyplot as plt import pandas as pd #### -- Step 1 (Download data)- URL_DATASET = r&#39;https://raw.githubusercontent.com/datasets/covid-19/master/data/countries-aggregated.csv&#39; df = pd.read_csv(URL_DATASET) df[&#39;NewCases&#39;] = df[&#39;Confirmed&#39;] - df[&#39;Confirmed&#39;].shift(1) #### -- Step 2 (Select data for India)- df_india = df[df[&#39;Country&#39;] == &#39;India&#39;] df_india.tail(5) . Date Country Confirmed Recovered Deaths NewCases . 25275 2020-11-28 | India | 9392919 | 8802267 | 136696 | 41810.0 | . 25276 2020-11-29 | India | 9431691 | 8847600 | 137139 | 38772.0 | . 25277 2020-11-30 | India | 9462809 | 8889585 | 137621 | 31118.0 | . 25278 2020-12-01 | India | 9499413 | 8932647 | 138122 | 36604.0 | . 25279 2020-12-02 | India | 9534964 | 8973373 | 138648 | 35551.0 | . #### -- Step 3 (Plot data)- # Increase size of plot plt.rcParams[&quot;figure.figsize&quot;]=20,14 # Remove if not on Jupyter df_india.plot( x = &#39;Date&#39;, y = &#39;Confirmed&#39;, color = &#39;blue&#39;) df_india.plot( x = &#39;Date&#39;, y = &#39;NewCases&#39;, color = &#39;blue&#39;) . &lt;AxesSubplot:xlabel=&#39;Date&#39;&gt; .",
            "url": "https://shashanksingh.github.io/blog_redux/covid/jupyter/python/2020/09/20/covid-impact.html",
            "relUrl": "/covid/jupyter/python/2020/09/20/covid-impact.html",
            "date": " • Sep 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "",
          "url": "https://shashanksingh.github.io/blog_redux/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://shashanksingh.github.io/blog_redux/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}